{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLB5Swe2ZPZR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Original text data\n",
        "data = \"\"\"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\"\"\"\n",
        "\n",
        "# Splitting the data into sentences\n",
        "sentences = data.split('.')\n",
        "clean_sent = []\n",
        "\n",
        "# Cleaning and preprocessing each sentence\n",
        "for sentence in sentences:\n",
        "    if sentence == \"\":\n",
        "        continue\n",
        "    # Remove non-alphanumeric characters\n",
        "    sentence = re.sub('[^A-Za-z0-9]+', ' ', sentence)\n",
        "    # Remove single characters and convert to lowercase\n",
        "    sentence = re.sub(r'(?:^| )\\w (?:$| )', ' ', sentence).strip()\n",
        "    sentence = sentence.lower()\n",
        "    clean_sent.append(sentence)\n",
        "\n",
        "# Tokenizing sentences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_sent)\n",
        "\n",
        "# Converting text to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(clean_sent)\n",
        "print(sequences)\n",
        "\n",
        "# Creating word-index and index-word dictionaries\n",
        "index_to_word = {}\n",
        "word_to_index = {}\n",
        "for i, sequence in enumerate(sequences):\n",
        "    word_in_sentence = clean_sent[i].split()\n",
        "    for j, value in enumerate(sequence):\n",
        "        index_to_word[value] = word_in_sentence[j]\n",
        "        word_to_index[word_in_sentence[j]] = value\n",
        "\n",
        "print(index_to_word, \"\\n\")\n",
        "print(word_to_index)\n",
        "\n",
        "# Setting vocabulary size and embedding dimensions\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "emb_size = 10\n",
        "context_size = 2\n",
        "\n",
        "# Generating context and target pairs\n",
        "contexts = []\n",
        "targets = []\n",
        "for sequence in sequences:\n",
        "    for i in range(context_size, len(sequence) - context_size):\n",
        "        target = sequence[i]\n",
        "        # Context words are the two words before and after the target word\n",
        "        context = [sequence[i - 2], sequence[i - 1], sequence[i + 1], sequence[i + 2]]\n",
        "        contexts.append(context)\n",
        "        targets.append(target)\n",
        "\n",
        "print(contexts, \"\\n\")\n",
        "print(targets)\n",
        "\n",
        "# Displaying some sample features with targets\n",
        "for i in range(5):\n",
        "    words = []\n",
        "    target = index_to_word.get(targets[i])\n",
        "    for j in contexts[i]:\n",
        "        words.append(index_to_word.get(j))\n",
        "    print(words, \" -> \", target)\n",
        "\n",
        "# Convert the contexts and targets to numpy arrays\n",
        "X = np.array(contexts)\n",
        "Y = np.array(targets)\n",
        "\n",
        "# Importing necessary libraries for building the model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    # Embedding layer to learn word embeddings\n",
        "    Embedding(input_dim=vocab_size, output_dim=emb_size, input_length=2*context_size),\n",
        "    # Lambda layer to average embeddings for each context\n",
        "    Lambda(lambda x: tf.reduce_mean(x, axis=1)),\n",
        "    # Hidden layers\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(512, activation='relu'),\n",
        "    # Output layer with softmax activation for multi-class classification\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X, Y, epochs=80)\n",
        "\n",
        "# Plotting the model's training history (accuracy/loss)\n",
        "import seaborn as sns\n",
        "sns.lineplot(history.history['loss'], label='Training Loss')\n",
        "sns.lineplot(history.history['accuracy'], label='Training Accuracy')\n",
        "\n",
        "# Reducing word embedding dimensions for visualization\n",
        "from sklearn.decomposition import PCA\n",
        "embeddings = model.get_weights()[0]\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(embeddings)\n",
        "\n",
        "# Test the model with example sentences\n",
        "test_sentences = [\n",
        "    \"known as structured learning\",\n",
        "    \"transformers have applied to\",\n",
        "    \"where they produced results\",\n",
        "    \"cases surpassing expert performance\"\n",
        "]\n",
        "\n",
        "for sent in test_sentences:\n",
        "    test_words = sent.split(\" \")\n",
        "    x_test = [word_to_index.get(i) for i in test_words]\n",
        "    x_test = np.array([x_test])\n",
        "\n",
        "    # Predict the next word based on context\n",
        "    pred = model.predict(x_test)\n",
        "    pred = np.argmax(pred[0])\n",
        "    print(\"Prediction for\", test_words, \"->\", index_to_word.get(pred), \"\\n\")\n"
      ]
    }
  ]
}